# 动手学习强化学习
## Chapter 1 初探强化学习
### 1.2 什么是强化学习
具体来讲，强化学习是机器通过与环境交互来实现目标的一种计算方法。\
所谓机器与环境的一轮交互，就是指机器在环境的一个状态下做一个动作的决策，把这个动作运用到环境中，这个环境发生相应的改变，并进一步将相应的奖励反馈和下一轮状态传回到机器中。这种交互往往是迭代进行的，机器的目标是，最大化在多轮交互过程中获得的累积奖励的期望。\
在强化学习中，常常采用智能体(Agent)的概念表示进行决策的机器，相比于有监督学习中的“模型”，强化学习的Agnet更强调机器不但可以感知到周围的环境信息，还可以通过决策直接改变这个环境，而不仅仅是进行信号的预测。\
具体而言，智能体和环境的交互，主要分为以下几个部分进行：
1. 智能体感知环境目前所处的状态，经过自身的计算，获取到对应的动作状态，并将其作用的环境中。
2. 环境得到智能体的动作后，产生即时的奖励信号，并进行相应的状态转移。

以此类推，智能体在每次交互中进行环境状态的感知，并对应获得相应的奖励，并进行状态转移。
对于智能体而言，制约其表现的关键要素包含三部分：
1. ***"感知"***: 智能体以何种方法感知周围环境的状态，从而知道自己所处的现状如何，下围棋的智能体如何感知自身的棋盘状况，无人车如何感知道路的相关状态，通过引入不同的特征感知，使得智能体可以学习相关环境的特征。
2. “决策”：智能体根据当前的状态计算出到达目标所需要采取的动作的过程叫做决策，举例来说，针对当前的棋盘，决定下一颗落子的位置，针对当前的路况，无人车计算出方向盘的角度和刹车、油门的力度等。策略是智能体最终体现出的智能形式，也是不同智能体之间的核心区别。
3. “奖励”：环境根据状态和智能体采取的动作，产生标量信号作为奖励反馈，利用标量信号，智能体能够更好的衡量这一轮动作的好坏。

基于上述表示，面向决策任务的强化学习，和面向预测任务的有监督学习，在形式上存在一定的区别，决策任务一般包含多轮交互，决策具有序列特性，前后存在一定关联，而预测任务总是单独的、独立的任务，假如决策任务转化为单轮决策，就可以转化为“判别最优动作”的预测任务。由于决策任务往往持续多轮，智能体就需要在每轮决策时考虑到未来环境的相应改变，一般而言，当前轮次下最大奖励反馈的动作，未必是最优的。

## Chapter 3 马尔可夫过程(Markov Decision Process, MDP)
### 3.1 马尔可夫过程
马尔可夫过程与随机过程密切相关，当且仅当某一时刻的状态仅取决于上一时刻的状态时，该随机过程就被称为具有***马尔可夫性质***
利用公式表示为
$P(S_{t+1} | S_t) =P(S_{t+1}|S_1S_2...S_{t})$
即下一时刻的状态仅取决于当前的状态，而不受到过去状态的影响

***具有马尔可夫性质的随机过程并不等于随机过程与历史完全没有关系，只是说明在进行邻近过程的转移时，下一时刻的状态仅受到当前状态的影响，但历史状态中的信息通过这种转移方式同样进行了传递***

一般而言，我们采用元组$<S,P>$描述马尔可夫过程，$S$为有限数量的集合，$P$为状态转移矩阵，给定一个确定的马尔可夫过程后，我们就可以从某个状态出发，根据状态转移矩阵生成状态序列，这个动作也被称之为采样，举例来说，如序列$s_1\to s_2 \to s_3 \to s_6$，生成序列的概率与转移矩阵相关，也与各个状态之间的转移概率密切相关。
### 3.2 马尔科夫奖励过程
基于马尔可夫过程的基础，额外引入奖励函数$r$和折扣因子$\gamma$,MDP过程就可以转化为MRP过程，即马尔科夫奖励过程***Markov Reward Process***,其可以用元组表示为$<S,P,r,\gamma>$,具体含义如下：
* S：有限状态的集合
* P：状态转移矩阵
* r：奖励因子，s的奖励r(s)代表转移到该状态时可以获得的奖励的期望
* $\gamma$：折扣因子，由于远期利益具有不确定性，有时我们更希望能够尽快获得部分奖励，所以需要对远期奖励进行折扣，该数值越接近于1，代表智能体更关注长期的累计奖励，而越接近0对应的更关注短期奖励。
对于一个MRP过程，从t过程的状态开始，到终止状态，所有奖励的衰减和被称为回报Return

$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2}+... = \sum_{k=0}^{\infty}{\gamma^kR_{t+k}}$

### 3.3 价值函数
在MRP过程中，一个状态的期望回报被称为这个状态的价值，所有状态的价值就组成了价值函数***value function***，价值函数的输入是某个特定的状态，输出为这个状态的价值。

$V(s) = E[G_t|S_t = s] = E[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2}+...|S_t = s] = E[R_t + \gamma V(S_{t+1})|S_t=s]$

最后的式子说明，即时奖励的一部分期望正是奖励函数的输出$r(s)$，而另一方面，等式中的剩余部分，同样可以从状态s出发的状态转移概率得到

$V(s) = r(s) + \gamma\sum_{s' \in S}{P(s'|s)V(s')}$

该方程即贝尔曼方程，在求解较大规模的马尔科夫奖励过程的价值函数时，可以采用动态规划、蒙特卡洛和时序差分算法。
### 3.4 Markov Decision Process
马尔可夫过程以及马尔可夫决策过程都是自发改变的随机过程，假如有一个外界的"刺激"共同改变这个随机过程，就变化为了MDP，即马尔可夫决策过程，这个外界的刺激就是智能体的动作***action***，
由此，在MRP的基础上增加action，我们就进一步获得了马尔可夫决策过程，用元组$<S,A,P,r,\gamma>$组成。
* S：状态的集合
* A：动作的集合
* $\gamma$：折扣因子，代表长期收益的追逐程度
* $r(s,a)$：奖励函数，共同取决于状态s和动作a，在奖励仅取决于状态s时，退化为r(s)
* $P(s'|s,a)$：状态转移函数，表示s执行动作a后到达s'的概率

在MDP定义中，我们不再使用MRP中的状态转移矩阵，直接采用了状态转移函数，原因有二：状态转移和动作相关，也和先前的状态有关，变成了三维数组，难以再采用矩阵的方式表示，

另一方面，状态转移函数更具有一般意义，***因为状态并不一定是离散的，假如状态集合并不有限，就无法采用数组表示，但仍然可以采用状态转移函数表示***

MDP过程中，一般有智能体与环境交互并获得动作的这个过程，智能体根据当前状态，从动作的集合A中选择一个动作的函数，被称之为***策略***，一般用如下公式表示

$\pi(a|s) = P(A_t=a|S_t=s)$

当策略是确定性策略，在每个状态下，函数只会输出一个确定的动作，当策略是随机性策略时，MDP输出的是一个概率分布，对该分布进行采样就可以获得一个动作。

为了衡量策略$\pi$的性能好坏，定义**状态价值函数(state-value function)**，定义从状态s出发遵循策略$\pi$能够获得的期望回报

$
V^\pi(s) = E_\pi[G_t | S_t =s]
$

同时还要引入动作对于状态转移的影响，因此引入**动作价值函数(action-value function)**,表示MDP遵循策略$\pi$时，对当前状态执行动作a所期望获得的回报。

$
Q^\pi(s,a) = E_\pi[G_t|S_t =s,A_t = a]
$

二者的关系为，在使用策略$\pi$时，状态$s$的价值等于在当前状态下，选择动作的概率以及相应的价值相乘再求和的结果

$
V^\pi(s) = \sum_{a \in A} {\pi(a|s)Q^\pi(s,a)}
$

在使用策略$\pi$时，在状态s下采取动作a的价值等于即时奖励，外加经过衰减后的，所有可能的下一个状态转移概率与相应价值的乘积。

$
Q^\pi(s,a) = r(s,a) + \gamma\sum_{s'\in S}P(s'|s,a)V^\pi(s')
$

通过推导，可以获得贝尔曼期望方程

$
Q^\pi(s,a)=E_\pi[R_t + \gamma Q^\pi(s',a') | S_t =s,A_t=a] = r(s,a) + \gamma \sum_{s'\in S}{P(s'|s,a)}\sum_{a' \in A}\pi(a'|s')Q^\pi(s'|a')
$

$
V^\pi(s) = E_\pi(R_t + \gamma V^\pi(s')|S_t = s) = \sum_{a\in A}\pi(a|s)(r(s,a)+\gamma\sum_{s' \in S}P(s'|s,a)V^\pi(s'))
$

### 3.5 蒙特卡罗方法
蒙特卡罗方法又被称为统计模拟方法，它是一种基于概率统计的数值计算方法，使用蒙特卡罗方法可以对策略在马尔可夫决策过程中的状态价值进行估计，状态的价值就是其对应的期望回报，假如我们采用MDP方法上采样多条序列，计算从该状态出发的汇报，求其数学期望，就可以比较简单的计算出状态的价值。

$
V^\pi(s) = E_\pi[G_t|S_t=s]\approx \frac{1}{N}\sum_{i=1}^{N}{G_t^{(i)}}
$

在序列中，可能从未出现过此状态，也有可能出现了不止一次，蒙特卡洛价值估计方法会在每一次状态出现时，分别计算其回报，同样的，也可以对于一条序列仅计算一次回报，在后续再次出现该状态时，将其进行忽略即可。

如何计算状态价值：
* 使用策略$\pi$采样若干条动作序列
* 对于每一条序列上，每一时间步$t$上的状态$s$进行以下操作：
    *  更新s的计数器 $N(s) +1 \to N(s)$
    *  更新状态s的总回报 $M(s) + G \to M(s)$
* 由此，每一个状态的价值就会被估计为回报的期望 $V(s) = M(s) / N(s)$

根据大数定律，当抽样的次数趋近于无限大时，期望估计就接近于实际状态的期望值，同样的，也可以采取类似的方法，对期望进行增量更新。

## Chapter 5 时序差分算法
### 5.1 时序差分简介
不同于传统的动态规划算法，动态规划算法要求马尔可夫决策过程必须已知，其要求与智能体交互的环境必须完全已知，比如迷宫、已经给定规则的网格世界，在这种条件下，智能体并不需要和环境交互进行数据采样，只需要通过规划算法求解最优价值策略，就像假如有监督学习给出了数据的分布公式，就可以在期望层面上直接最小化模型的泛化误差进行参数的更新，而并不需要对数据点进行采样。

***然而，机器学习的主要方法大多是对数据分布未知的情况下，针对具体的数据点进行模型更新，对于大部分强化学习的现实场景，马尔科夫决策中的P函数无法写出，因此无法直接进行动态规划。*** 因此，此时智能体只能与环境进行交互，通过采样到的数据进行学习，因而也无法直接进行动态规划，这类学习方法，也被称为无模型的强化学习。

区别于动态规划算法，无模型强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样得到的数据进行学习，从而可以被使用到简单的实际场景中。***Sarsa***和***Q-Learning***算法都是这种模式下的经典算法，基于时序差分TD提出。

### 5.2 时序差分
时序差分是一种用来估计策略价值函数的方法，结合了蒙特卡洛和动态规划算法的思想，时序差分方法和蒙特卡洛的相似之处在于，可以从样本中进行学习，而不需要事先知道环境，和动态规划的相似之处在于可以根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。

在蒙塔卡罗方法中，增量更新方式如下：

$V(s_t)\gets V(s_t)+\alpha [G_t - V(s_t)]$

实际上，就是将增量更新中的$1/N(s)$修改为$\alpha$，表示其对于价值估计更新的步长，由此，我们可以将其取作一个常数，也就不需要像蒙特卡罗方法一样，必须要等整个序列采样结束，才需要计算这一次的回报$G_t$，而时序差分算法只需要当前步结束即可进行计算。具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态可以获得的回报：

$V(s_t)\gets V(s_t) + \alpha[r_t + \gamma V(s_{t+1}) - V(s_t)]$

其中，$[r_t + \gamma V(s_{t+1}) - V(s_t)]$被称为时序差分误差，时序差分算法将其与步长的乘积作为状态价值的更新量。

### 5.3 Sarsa算法
我们可以采用时序差分方法估计状态价值函数，同样的，是否可以类似的方法进行策略迭代，实现强化学习，策略评估已经可以通过时序差分算法实现，在不知道奖励函数和状态转移函数的情况下，如何进行策略提升。

采用与时序差分相似的思想，对于动作价值函数Q，我们可以直接进行如下估计

$Q(s_t,a_t) \gets Q(s_t,a_t) + \alpha[r_t + \gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$

采用贪婪算法选择在某状态下，动作价值最大的动作，即$argmax_aQ(s,a)$，这样就可以实现贪婪算法根据价值进行动作选取和环境交互，并根据得到的数据，利用时序差分算法更新动作价值的估计值。

这样的思想存在两个需要考虑的问题，其一，时序差分算法进行准确估计策略的状态价值函数，需要大量的样本进行更新，但可以基于部分样本进行策略评估，随后就可以进行策略的更新，因为策略提升可以在评估未完全结束的前提下就进行，这一思想被称为***广义策略迭代(generalized policy iteration)***

其二，如果在策略提升的过程中，一直根据贪婪算法进行确定性策略的获得，可能会导致状态动作对中的某一部分$(s,a)$一直没有出现，导致无法对其价值进行估计，因此，相比于选择简单的贪婪算法，我们更期望选择$\epsilon$-贪婪算法，采取随机采取动作的方式，实现对尽可能多的动作的采样。基于上述思想，我们最终确定了Sarsa算法的基本架构如下：

```python
input: 当前状态s、当前动作a、获得奖励r、下一状态s'、下一动作a'
初始化Q(s,a)
for 序列e = 1~E do:
    获取初始状态s
    采用贪婪算法基于Q选择当前状态s下的动作a
    for timestept=1 ~ T do:
        获取环境反馈r,s'
        采用贪婪算法基于Q选择s'下的动作a'
        Q(s,a) = Q(s,a) + a[r + yQ(s',a') - Q(s,a)]
        s = s'
        a = a'
    end for
end for
```
### 5.4 多步Sarsa算法
蒙特卡罗方法利用当前状态之后的每一步奖励，而不使用任何的价值估计，时序差分方法则只利用一步奖励和下一个状态的价值估计。从总体而言，其整体的区别为，蒙塔卡罗方法为无偏估计，但会出现较为明显的方差，这是因为，每一步的状态转移都会有不确定性，每一步状态采取的动作，所得到的不一样的奖励最终也都会加在一起，这会极大影响最终的价值估计。时序差分算法则是小方差的方法，因为它只关注一步状态转移，只用一步的奖励信息，但是它是有偏估计，因为下一个状态的价值估计并不是其精准的价值。

为了充分使用两个方法的优势，我们使用了**多步时序差分**，就是充分使用n步奖励，在之后使用状态的价值进行估计，公式如下

$G_t = r_t + \gamma r_{t+1} + ... + \gamma^n Q{(s_{t+n},a_{t+n})}$

由此，也可以将动作价值函数的更新公式调整为如下方案：

$Q(s_t,a_t) \gets Q(s_t,a_t) + \alpha[r_t + \gamma r_{t+1} + ... + \gamma^nQ(s_{t+1},a_{t+1})-Q(s_t,a_t)]$

### 5.5  Q-learning算法
除Sarsa算法外，经典的使用时序差分算法还包括Q-learning，其与Sarsa算法的最大区别为，Q-learning的时序差分更新方法如下:

$Q(s_t,a_t) \gets Q(s_t,a_t) + \alpha[r_t + \gamma max_aQ(s_{t+1},a) - Q(s_t,a_t)]$

我们可以先回顾动作价值函数的贝尔曼最优方程：
$Q^*(s,a) = r(s,a) + \gamma \sum_{s'\in S}{P(s'|s,a)\max_{a' \in A}{Q^*(s',a')}}$

sarsa的做法为，基于贪婪策略进行动作价值函数的估计，而Q-learning算法直接进行Q*的估计。
但是Q-learning并不一定必须使用当前贪婪策略抽样出的数据进行更新，因为只要给定s,a,r,s'就可以直接根据更新公式进行Q的更新，为了探索，我们会选择贪婪算法进行与环境的交互，而Sarsa算法必须使用严格的s,a,r,s',a'进行更新，这源于其更新算法中用到了当前状态s'下的动作a'，因此，我们称Sarsa为在线策略算法，Q-learning为离线策略算法。

#### 在线策略算法和离线策略算法
采样数据的策略被称为行为策略(behavior policy)，利用这些数据进行更新的策略为目标策略(target policy)。在线策略算法表示行为策略和目标策略是相同的策略，而离线算法则可以允许二者之间存在差异，***二者之间的核心区别是，看更新时序差分价值目标的数据是否来自于当前的策略***

对于Sarsa而言，更新公式必须严格依靠当前策略采样得到的五元组(s,a,r,s',a')，因此是典型的***在线策略算法***。

对于Q-learning而言，更新公式使用(s,a,r,s')来更新当前状态动作对的价值Q(s,a)，而a'由max函数得出，因此并不一定要求必须是当前策略采样得到的数据，因而是***离线策略算法***。

***离线策略算法能够重复使用过往的训练样本，因而具有更小的样本复杂度***

虽然离线策略学习可以允许智能体基于经验回放池中的样本进行学习，但是需要保证智能体在学习的过程中可以不断和环境之间进行交互，将采样得到的最新经验样本加入经验回放池中，从而使其与当前智能体策略对应的数据分布保持较近的距离，假如不允许智能体在学习过程中与环境持续进行交互，而是直接选择给定的样本集直接进行策略训练，这样的学习模式就被称为***离线强化学习(Offline Reinforcement Learning)***

## Chapter 7 DQN算法
### 7.1 DQN算法简介：
先前的Sarsa算法和Q-learning算法中，我们都按照矩阵的方式，存储了每个状态下所有的动作对应的价值表格，在表格中，每一个动作价值Q(s,a)表示，在状态s下选择动作a然后继续遵循某一策略期望得到的回报，但是，这种用表格存储动作价值的做法存在两个问题，其一，环境中状态和动作未必是离散的，对于连续的状态和动作而言，这种表格的方法显然是不适用的。其二，即使状态和动作离散，当环境的动作域和状态域都比较大时，这样的做法同样会导致极大的计算复杂度。

对于上述的两种情况，我们都需要利用函数拟合的方法来完成对Q值的估计，将这个复杂的Q值表格作为数据，使用参数化的函数Q进行数据的拟合，但由于这种方法显然会存在误差，因此属于近似方法，DQN算法就可以用于解决连续状态下的离散动作问题。

### 7.2 车杆环境
以车杆环境这种经典的强化学习环境为例，其状态值连续，而动作则是离散的，智能体的任务是，通过左右移动保持车上的杆子维持竖直，杆的倾斜度数过大的话，或者车子距离初始位置左右的偏离程度过大，都会导致游戏结束。

智能体的状态是一个维数为4的向量，每一维都保持连续，动作是离散的，动作空间大小为2，即，向左移动和向右移动，在游戏中，每当小车坚持1帧，就可以获得分数为1的奖励，坚持时间越长，对应最后的分数也能越高。

### 7.3 DQN
为了在类似车杆的环境中，即状态域是连续的，这样的环境中期望获得动作价值函数Q(s,a)，由于状态的每个维度都是连续的，无法采用表格进行记录，因此，常用的一个解决办法就是使用函数拟合，由于神经网路能够具有强大的表达能力，因此，我们可以设计一个神经网络来表示函数Q。

假如动作是连续的，神经网络的输入是状态s和动作a，输出一个标量表示价值。

假如动作是离散的，我们还可以只将状态s输入到神经网络中，使其同时输出每一个动作的Q值。

一般而言，DQN算法和Q-learning只能处理***动作离散***的情况,这源于其在进行动作价值函数更新时出现的max操作。

假设神经网络用来拟合函数Q的参数为w，对于每一个状态s下所有可能动作a的Q值都有表示$Q_\omega (s,a)$。这样的神经网络就可以被称之为Q网络。

基于先前的分析，我们知道，Q-learning采用时序差分算法进行目标$r + \gamma \max_{a' \in A} Q(s',a')$来进行对于$Q(s,a)$的增量式更新，也就是说，需要使$Q(s,a)$向时序差分误差目标靠近，因此，对于给定的数据${(s_i,a_i,r_i,s_i')}$，一个非常自然的思想就是将其构建成均方误差的形式。

$\omega^* = \argmin_{\omega} \frac{1}{2N}\sum_{i=1}^{N} [Q_\omega(s_i,a_i) - (r_i + \gamma \max_{a'\in A}Q_\omega(s',a'))]^2$

基于这种思想，我们就实现了对于Q-learning算法的神经网络形式_深度Q网络(deep Q network, DQN)算法。由于其为离线决策算法，因此，在收集相应数据时，可以采用贪婪算法进行探索和利用之间的平衡，DQN两个关键的模块分别为经验回放和目标网络，能够帮助DQN取得更加稳定和出色的性能。

#### 7.3.1 经验回放
在有监督学习中，假设训练数据之间独立同分布，我们每次训练神经网络的时候，就会从数据中抽样若干数据进行梯度下降，随着学习的不断进行，每一个训练数据都会被使用多次，在原先的Q-learning算法中，每一个数据只会用于更新一次Q值，为了更好地将Q-learning和深度神经网络结合，DQN选择了经验回放方法，其选择进行回放缓冲区的维护，将每一次从环境中采样获得的$<s_i,a_i,r_i,s_i+1>$存储到回放缓冲区中，在训练Q网络的过程中，再从回放缓冲区中随机采样若干数据进行训练。

这样的做法存在两个作用：
1. 样本满足独立假设。在MDP中交互采样所获得的数据本身并不满足独立性假设，这是因为当前状态与上一时刻的状态相关，非独立同分布的数据对于训练神经网络具有较大的影响，使其容易被拟合到最近一次训练的数据之上，采用经验回放可以打破样本之间的相关性，使其满足独立性假设。
2. 提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习算法。

#### 7.3.2 目标网络
DQN算法最终更新的目标是，使得$Q_\omega(s,a)$尽可能地逼近$r+\gamma \max_{a'}Q_\omega(s',a')$,由于TD误差目标本身也包含着神经网络的输出，因此，在更新网络参数的同时，目标也在不断的改变，这很有可能会导致在神经网络训练过程中的不确定性，为了解决这一问题，DQN选择目标网络的思想，既然Q网络的不断更新会导致目标不断发生改变，那么就可以将TD误差目标中的Q网络先保持不变，因此需要两套Q网络来实现这一目标。
1. 对于原先的训练网络$Q_\omega(s,a)$，用于计算原先的损失函数中$Q_\omega(s,a)$项，并采用正常的梯度下降方法进行更新。
2. 对于目标网络$Q_\omega^*(s,a)$，用于计算TD项$r+\gamma \max_{a'}Q_\omega(s',a')$，其中，$\omega ^*$表示目标网络中的相关参数，如果网络中的参数随时保持一致，那么网络的稳定性仍然无法增强，为了让更新目标更稳定，目标网络并不会每一步都进行更新，具体而言，目标网络使用了较为陈旧的一套参数，训练网络则会在训练中的每一步都进行更新，而目标网络的参数间隔C步才会同步一次，这样就允许其在训练过程中保持较高的稳定性。

## Chapter 10 Actor-Critic网络
### 10.1 AC网络简介
DQN网络的思想是，基于环境特征直接学习价值函数进行预测，而也有从策略函数上入手，调整策略函数的取值，尽可能地进行目标拟合的思想。因此，一个自然的问题就是，有没有什么方法，既可以学习到价值函数，又可以学习到策略函数。Actor-Critic网络就可以实现这一目标，目前而言，许多优秀的算法都基于Actor-Critic算法进行研发。

***需要注意的是，Actor-Critic算法本质上是一种基于策略的算法，因为这一系列算法的目标都是优化一个带有参数的策略，只是需要额外进行学习价值函数，从而帮助策略函数更好地进行学习***

### 10.2 Actor-Critic
在REINFORCE算法中，目标函数的梯度中存在轨迹回报，用于指导策略的更新，利用蒙特卡罗方法进行Q(s,a)进行参数估计，那么，是否可以采用值函数的拟合来指导策略进行学习，这正是AC网络所致力于做到的事情。
在先前的学习中，我们知道，蒙特卡罗方法是一种无偏估计的方法，但会引入较大的方差，为了减小方差，我们可以引入基线函数减小方差，也可以利用AC网络来进行动作价值函数的估计，代替蒙特卡罗方法的抽样结果。

常用的一种思想为，采用时序差分残差进行指导策略梯度的学习，即计算$r_t + \gamma V^{\pi}(s_{t+1}) - V^\pi(s_t)$，实际上，利用Q值或着V值从本质上来讲也是利用奖励来进行指导，但是利用神经网络进行估计的方法可以减小方差，提升模型的鲁棒性。除此之外，REINFORCE算法基于蒙特卡洛采样的思想进行更新，只允许在每一次序列结束后才能进行更新操作，也就说明了任务必须要有有限的步数，而Actor-Critic算法则可以在每一步中进行更新，并且不对任务的步数做限制。

AC网络主要分两部分，Actor策略网络和Critic价值网络，Actor网络主要负责与环境之间进行交互，并且在价值函数的指导下基于策略梯度学习一个更好的策略。Critic基于Actor与环境交互收集到的数据中学习到一个价值函数，这个价值函数帮助判断在当前状态下，什么动作是好的，什么动作不是好的，从而帮助Actor进行策略更新。

Actor采用策略梯度原则进行更新，而Critic是如何更新的，不妨取价值网络为$V_{\omega}$,对应参数为$\omega$，基于时序差分残差的学习方式，对于单个数据定义如下价值函数的损失函数：

$L(\omega) = \frac{1}{2}(r+\gamma V_\omega(s_{t+1})-V_\omega(s_t))^2$

与DQN类似，我们采用了类似目标网络的方法，将上式中的$r+\gamma V_\omega(s_{t+1})$作为时序差分目标，不会产生梯度来更新价值函数，因此，价值函数的梯度可以写成

$\Delta_\omega L(\omega) = - (r+\gamma V_\omega(s_{t+1}) - V_\omega(s_t))\Delta_\omega V_\omega(s_t)$

随后，按照梯度下降方法更新Critic价值网络参数即可。

## Chapter 11 TRPO算法
### 11.1 简介：
基于策略的方法包括策略梯度算法和Actor-Critic算法，这些算法虽然简单、直观，但在实际应用的过程中会遇到训练不稳定的情况。基于策略的方法需要实现对与智能体的参数化，并且衡量策略好坏的目标函数，通过梯度上升的方法来最大化这个目标函数，最终使其策略最有。具体来说，假设$\theta$表示策略$\pi_\theta$的相关参数，定义

$J(\theta) = E_{\pi_\theta}[\sum_{t=0}^{\infty}{\gamma^tr(s_t,a_t)}]$

基于策略的方法是为了找到一组参数$\theta^* = arg \max_{\theta}J(\theta)$策略梯度算法主要沿着梯度方向迭代更新策略参数，但是这种算法存在一种明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长导致策略突然变差，进而导致训练效果受到影响。

## Chapter 12 PPO算法
### 12.1  简介
TRPO算法在很多场景上的应用都非常成功，但是其计算过程也呈现出了非常复杂的特征，每一步更新时，运算量都是非常大的，因此，PPO算法作为TRPO的优化算法被移除，基于TRPO的思想，但更加简单的实现了相关算法，大量的实验结果表明，相比于TRPO算法，PPO算法能够更快的达到相同程度下的学习，假如我们期望在新环境中进行强化学习算法，PPO就可以首先进行算法尝试。

在优化策略函数时，TRPO通过泰勒展开近似、共轭梯度和线性搜索等方法直接进行求解，而PPO算法的优化目标与其保持一致，但选择了更加简单的方法进行求解，具体而言，PPO算法包含PPO-惩罚、PPO-截断。
### 12.2  PPO-惩罚
PPO-惩罚 ***(PPO-Penalty)*** 采用拉格朗日乘数法，直接将KL散度的限制放入目标函数中，从而将其变化为无约束的优化问题，在迭代的过程中不断更新KL散度前的系数。

$arg_\theta\max E_{s-v^{\pi \theta_k}}E_{a-\pi\theta_k(·|s)}[]$
### 12.3 PPO-截断
PPO的另一种形式PPO-截断(PPO-Clip)更加直接，通过在目标函数中直接进行限制，实现保持新参数和旧参数之间的差距不会太大。